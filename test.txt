# -*- coding: utf-8 -*-
import requests
import os
from openpyxl import Workbook
from openpyxl import load_workbook
from bs4 import BeautifulSoup
import time

def get_all_xian_qu_urls():
    base_url='http://chongqing.11467.com/'
    header= {
                'Connection':'close',
                'User-Agent':'Mozilla/5.0 (Windows; U; Windows NT 6.1; en-US; rv:1.9.1.6) Gecko/20091201 Firefox/3.5.6'
                }
    res=requests.get(base_url,headers=header,timeout=(3.05, 27))
    soup = BeautifulSoup(res.content,'html.parser')
    with open('test.html','w') as f:
        f.write(res.text)
    all_dt_tag=soup.find_all('dt')
    #print(all_dt_tag)
    for dt in all_dt_tag:
        if '梁平县' in dt.get_text():
            print('梁平县')
            i=all_dt_tag.index(dt)
        if '璧山县' in dt.get_text():
            print('璧山县')
            j=all_dt_tag.index(dt)
    xian_qu_urls=[]
    for index in range(i,j):
        xian_qu_urls.append(all_dt_tag[index].find('a').get('href'))
    return xian_qu_urls
def get_xian_qu_all_page_urls(quxians_url):
    header= {
                'Connection':'close',
                'User-Agent':'Mozilla/5.0 (Windows; U; Windows NT 6.1; en-US; rv:1.9.1.6) Gecko/20091201 Firefox/3.5.6'
                }
    #获取页数
    page_urls=[]
    for quxian_url in quxians_url:
        try:
            res=requests.get(quxian_url,headers=header,timeout=(3.05, 27))
            soup = BeautifulSoup(res.content,'html.parser')
            pages=soup.find_all('div',{'class':'pages'})
            #print(pages)
            if pages:
                max_page_num=pages[0].find_all('a')[-1].get('href').replace(quxian_url+'pn','')
                for num in (1,int(max_page_num)):
                    page_urls.append(quxian_url+'pn'+str(num))
            else:
                print(quxian_url+':分页url获取失败')
            time.sleep(5)
        except Exception as e:
            print(e)
            print(quxian_url+':分页url获取失败')
        
    return page_urls
def get_xianqu_page_company_name(pages_url,filename):
    header= {
                'Connection':'close',
                'User-Agent':'Mozilla/5.0 (Windows; U; Windows NT 6.1; en-US; rv:1.9.1.6) Gecko/20091201 Firefox/3.5.6'
                }
    path=os.path.join(os.path.abspath('.'),filename+'.xlsx')
    if os.path.exists(path):
        wb = load_workbook(self.path)
        ws1=wb.get_sheet_by_name('公司名字')
    else:
        wb = Workbook()
        ws1=wb.create_sheet('公司名字',0)
    for page_url in pages_url:
        try:
            print('第{index}页公司爬取中.....'.format(index=pages_url.index(page_url)))
            res=requests.get(page_url,headers=header,timeout=(3.05, 27))
            soup = BeautifulSoup(res.content,'html.parser')
            company_list=soup.find_all('ul',{'class':'companylist'})
            all_li_tag=company_list[0].find_all('li')
            for li in all_li_tag:
                company_name=li.find('h4').get_text()
                row_max=ws1.max_row
                ws1.cell(row =row_max+1, column =1, value=company_name)
            time.sleep(5)
        except Exception as e:
            print(e)
            print(page_url+':页面爬取失败.')
if __name__=='__main__':
    quxians_url=get_all_xian_qu_urls()
    pages_url=get_xian_qu_all_page_urls(quxians_url)
    get_xianqu_page_company_name(pages_url,'重庆公司')
    
# -*- coding: utf-8 -*-
import PIL.Image as image
import PIL.ImageChops as imagechops
import time,re,io,urllib,random
from selenium import webdriver
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.common.action_chains import ActionChains
from selenium.webdriver.common.keys import Keys  
from bs4 import BeautifulSoup
import requests
import os
from openpyxl import Workbook
from openpyxl import load_workbook


class spider():
    def __init__(self,filename=r'重庆公司信息(工商局版)'):
        self.path=os.path.join(os.path.abspath('.'),filename+'.xlsx')
        if os.path.exists(self.path):
            self.wb = load_workbook(self.path)
            self.ws1 = self.wb.get_sheet_by_name('公司信息')
            self.ws2 = self.wb.get_sheet_by_name('未爬到的公司名字')
        else:
            self.wb = Workbook()
            self.ws1=self.wb.create_sheet('公司信息',0)
            self.ws2=self.wb.create_sheet('未爬到的公司名字',1)
        self.ws1.cell(row =1, column =1, value='公司名字')
        self.ws1.cell(row =1, column =2, value='统一社会信誉代码')
        self.ws1.cell(row =1, column =3, value='公司类型')
        self.ws1.cell(row =1, column =4, value='法定代表人')
        self.ws1.cell(row =1, column =5, value='注册资本')
        self.ws1.cell(row =1, column =6, value='成立日期')
        self.ws1.cell(row =1, column =7, value='核准日期')
        self.ws1.cell(row =1, column =8, value='登记机关')
        self.ws1.cell(row =1, column =9, value='登记状态')
        self.ws1.cell(row =1, column =10, value='营业场所')
        self.ws1.cell(row =1, column =11, value='经营范围')
        
        self.ws2.cell(row =1, column =1, value='公司名字')
        self.wb.save(self.path)
    def get_merge_image(self,filename,location_list):
        im = image.open(filename)
        new_im = image.new('RGB', (260,116))
        im_list_upper=[]
        im_list_down=[]
        for location in location_list:
            if location['y']==-58:
                im_list_upper.append(im.crop((abs(location['x']),58,abs(location['x'])+10,166)))
            if location['y']==0:
                im_list_down.append(im.crop((abs(location['x']),0,abs(location['x'])+10,58)))
        new_im = image.new('RGB', (260,116))
        x_offset = 0
        for im in im_list_upper:
            new_im.paste(im, (x_offset,0))
            x_offset += im.size[0]
        x_offset = 0
        for im in im_list_down:
            new_im.paste(im, (x_offset,58))
            x_offset += im.size[0]    
        return new_im
    def get_image(self,driver,div):
        #'''    下载并还原图片    :driver:webdriver    :div:图片的div    '''
        #找到图片所在的div
        header= {
            'Connection':'close',
            'User-Agent':'Mozilla/5.0 (Windows; U; Windows NT 6.1; en-US; rv:1.9.1.6) Gecko/20091201 Firefox/3.5.6'
            }
        background_images=driver.find_elements_by_xpath(div)
        location_list=[]
        imageurl=''
        for background_image in background_images:
            location={}
            #在html里面解析出小图片的url地址，还有长高的数值
            location['x']=int(re.findall("background-image: url\(\"(.*)\"\); background-position: (.*)px (.*)px;",background_image.get_attribute('style'))[0][1])
            location['y']=int(re.findall("background-image: url\(\"(.*)\"\); background-position: (.*)px (.*)px;",background_image.get_attribute('style'))[0][2])
            imageurl=re.findall("background-image: url\(\"(.*)\"\); background-position: (.*)px (.*)px;",background_image.get_attribute('style'))[0][0]
            location_list.append(location)
            imageurl=imageurl.replace("webp","jpg")
            #print(imageurl)
            try:
				#req=urllib.request.urlopen(imageurl)
				#req.read()
                jpgfile=io.BytesIO(requests.get(imageurl,headers=header,timeout=(3.05, 27)).content)
            except Exception as e:
                #req.close()
                print('获取滑动验证图片失败,重新获取!')
                print('*'*20)
                print(e)
                print('*'*20)
                return False
            #重新合并图片
			#req.close()
            image=self.get_merge_image(jpgfile,location_list )
        return image
            
    def is_similar(self,image1,image2,x,y):
        #'''    对比RGB值    '''
        pixel1=image1.getpixel((x,y))
        pixel2=image2.getpixel((x,y))
        for i in range(0,3):
            if abs(pixel1[i]-pixel2[i])>=50:
                return False
        return True
    def get_diff_location(self,image1,image2):
        #'''    计算缺口的位置    '''
        i=0
        for i in range(0,260):
            for j in range(0,116):
                if self.is_similar(image1,image2,i,j)==False:
                    return  i,j
    def get_track(self,length):
        list=[]
        #间隔通过随机范围函数来获得,每次移动一步或者两步
        x=random.randint(2,4)
        #生成轨迹并保存到list内
        total=length-5
        #step1=total//2
        step1=total//4*3
        list.append(step1)
        step2=(total-step1)//4*3
        list.append(step2)
        total2=total-step1-step2
        while total2-x>5:
            list.append(x)
            total2=total2-x
            x=random.randint(2,5)
        list.append(total2)
        return list
    def hk_veriy(self,driver):
        WebDriverWait(driver, 30).until(lambda the_driver: the_driver.find_element_by_xpath("//div[@class='gt_cut_bg gt_show']").is_displayed())
        WebDriverWait(driver, 30).until(lambda the_driver: the_driver.find_element_by_xpath("//div[@class='gt_cut_fullbg gt_show']").is_displayed())
        WebDriverWait(driver, 30).until(lambda the_driver: the_driver.find_element_by_xpath("//div[@class='gt_slider_knob gt_show']").is_displayed())
        image1_xpath="//div[@class='gt_cut_bg gt_show']/div"
        image2_xpath="//div[@class='gt_cut_fullbg gt_show']/div"
        image1=self.get_image(driver,image1_xpath)
        image2=self.get_image(driver,image2_xpath)
        if not(image1 and image2):
            return False
        #圆球位置
        element=driver.find_element_by_xpath("//div[@class='gt_slider_knob gt_show']")
        location=element.location
        #缺口位置
        diff_location=self.get_diff_location(image1,image2)
        #x轨迹
        track_list=self.get_track(diff_location[0])
        #按住圆球
        ActionChains(driver).click_and_hold(on_element=element).perform()
        time.sleep(0.15)
        #x，y偏移
        t1=time.time()
        #注意鼠标抖动范围5像素
        i=0
        j=1
        f=0
        for track in track_list:
            yoffset=random.randint(22,25)
            if len(track_list)<7:
                j=1.5
            if len(track_list)>15:
                j=0.5
            if i>len(track_list)//len(track_list)*(len(track_list)-1):
                k=4
            elif i>len(track_list)//2:
                k=2
            else:
                k=1
            i=i+1
            ActionChains(driver).move_to_element_with_offset(to_element=element, xoffset=track+22, yoffset=yoffset).perform()
            time.sleep(random.randint(40,50)/500*k*j)
        ActionChains(driver).move_to_element_with_offset(to_element=element, xoffset=22+2, yoffset=yoffset).perform()
        time.sleep(random.randint(50,60)/500)
        ActionChains(driver).move_to_element_with_offset(to_element=element, xoffset=22-2, yoffset=yoffset).perform()
        time.sleep(random.randint(50,60)/500)
        ActionChains(driver).release(on_element=element).perform()
        try:
            WebDriverWait(driver, 15).until(lambda the_driver: the_driver.find_element_by_xpath("//div[@class='gt_ajax_tip gt_success']").is_displayed())
            #driver.find_element_by_xpath("//div[@class='gt_ajax_tip gt_success']").is_displayed()
        except Exception as e:
            #print(e)
            print('滑动验证失败,重新验证。')
            return False
        return True
    def get_all_company(self,driver,cookies):
        html=driver.page_source
        soup=BeautifulSoup(html,'html.parser')
        page_form=soup.find_all('div',{'class':'pagination'})
        print(page_form)
        if page_form:
            page_a_list=page_form[0].find_all('a')
            a_list=soup.find_all('a',{'class':'search_list_item db'})
            #print(a_list)
            for link in a_list:
                name=link.find('h1').get_text()
                self.html_parser(link.get('href'),cookies,name)
            if len(page_a_list)>2:
                for page in range(len(page_a_list)-2):
                    #下一页
                    old_url=driver.current_url
                    if page:
                        k=2
                    else:
                        k=0
                    try:
                        driver.find_element_by_xpath('/html/body/div[5]/div[3]/div[2]/form/a[{next_page}]'.format(next_page=len(page_a_list)-1+k)).click()
                    except:
                        try:
                            driver.find_element_by_link_text('{page}'.format(page=page+2))
                        except:
                            break
                    i=0
					#翻页等待30秒,每10秒重新翻页!
                    while driver.current_url==old_url:
                        i=i+1
                        time.sleep(1)
                        if i%10:
                            try:
                                driver.find_element_by_link_text('{page}'.format(page=page+2))
                            except:
                                pass
                        if i==30:
                            print('翻页失败,跳出循环爬取下一页!')
                            break
                    if 'corp-query-search' not in driver.current_url:
                        driver.quit()
                        return
                    html=driver.page_source
                    soup=BeautifulSoup(html,'html.parser')
                    a_list=soup.find_all('a',{'class':'search_list_item db'})
                    for link in a_list:
                        name=link.find('h1').get_text()
                        self.html_parser(link.get('href'),cookies,name)
        else:
            print('该关键字没有匹配的公司!')
        driver.quit()
    def html_parser(self,url,cookies,name):
        url='http://www.gsxt.gov.cn'+url
        header= {
            'Connection':'close',
            'User-Agent':'Mozilla/5.0 (Windows; U; Windows NT 6.1; en-US; rv:1.9.1.6) Gecko/20091201 Firefox/3.5.6'
            }
        result={}
        try:
            print('开始获取公司:{name}————————>'.format(name=name))
            res=requests.get(url,headers=header,cookies=cookies,timeout=(3.05, 27))
            print('开始解析公司:{name}————————>'.format(name=name))
            soup = BeautifulSoup(res.content,'html.parser')
            fullName_tag=soup.find(attrs={'class':'fullName'})
            if fullName_tag:
                info_div=soup.find(attrs={'class':'overview'})
                info_dl=info_div.find_all('dl')
                result['公司名字: ']=fullName_tag.get_text().replace('\t','').replace('\r','').replace('\n','').strip()
                for dl in info_dl:
                    print(dl)
                    dict_name=dl.find_all('dt')[0].get_text().replace('\t','').replace('\r','').replace('\n','').strip()
                    dict_value=dl.find_all('dd')[0].get_text().replace('\t','').replace('\r','').replace('\n','').strip()
                    result[dict_name]=dict_value
                    #print(result)
                self.write_info_excel(result)
            else:
                #print('not dound',name)
                print('解析公司失败:{name}————————>'.format(name=name))
                self.ws2.cell(row =self.ws2.max_row+1, column =1, value=name)
                self.wb.save(self.path)
        except Exception as e:
            print('解析公司失败:{name}————————>'.format(name=name))
            print('*'*20)
            print(e)
            print('*'*20)
            self.ws2.cell(row =self.ws2.max_row+1, column =1, value=name)
            self.wb.save(self.path)
    def write_info_excel(self,result):
        row_max=self.ws1.max_row
        for name,value in result.items():
            if name=='公司名字: ':
                self.ws1.cell(row =row_max+1, column =1, value=value)
            if name=='统一社会信用代码：' or name=='注册号：':
                self.ws1.cell(row =row_max+1, column =2, value=value)
            if name=='类型：':
                self.ws1.cell(row =row_max+1, column =3, value=value)
            if name=='法定代表人：' or name=='负责人：':
                self.ws1.cell(row =row_max+1, column =4, value=value)
            if name=='注册资本：':
                self.ws1.cell(row =row_max+1, column =5, value=value)
            if name=='成立日期：':
                self.ws1.cell(row =row_max+1, column =6, value=value)
            if name=='核准日期：':
                self.ws1.cell(row =row_max+1, column =7, value=value)
            if name=='登记机关：':
                self.ws1.cell(row =row_max+1, column =8, value=value)
            if name=='登记状态：':
                self.ws1.cell(row =row_max+1, column =9, value=value)
            if name=='营业场所：' or name=='住所：':
                self.ws1.cell(row =row_max+1, column =10, value=value)
            if name=='经营范围：':
                self.ws1.cell(row =row_max+1, column =11, value=value)
        self.wb.save(self.path)     
    def search_keyword_parser_write(self,keyword):
        driver=webdriver.Firefox()
        time.sleep(2)
        try:
            driver.get("http://www.gsxt.gov.cn/index.html")
            try:
                WebDriverWait(driver, 30).until(lambda the_driver: the_driver.find_element_by_id("keyword").is_displayed())
                WebDriverWait(driver, 30).until(lambda the_driver: the_driver.find_element_by_id("btn_query").is_displayed())
            except:
                driver.get("http://www.gsxt.gov.cn/index.html")
                WebDriverWait(driver, 30).until(lambda the_driver: the_driver.find_element_by_id("keyword").is_displayed())
                WebDriverWait(driver, 30).until(lambda the_driver: the_driver.find_element_by_id("btn_query").is_displayed())
            driver.find_element_by_id('keyword').send_keys(keyword)
            driver.find_element_by_id('btn_query').click()
            old_url=driver.current_url
            t=0
            while not self.hk_veriy(driver):
                time.sleep(2)
                t=t+1
                print('滑动验证失败,第{tt}次,重新认证!'.format(tt=t))
                if t==7:
                    print('滑动验证失败,超过7次,跳出关键字:{key}!'.format(key=keyword))
                    driver.quit()
                    return
            t=0
            while driver.current_url==old_url:
                print('等待跳转成功!')
                time.sleep(1)
                t=t+1
                if t==50:
                    print('跳转失败,爬取下一个关键字!')
                    driver.quit()
                    return
            if 'corp-query-search' not in driver.current_url:
                print('跳转失败,爬取下一个关键字!')
                driver.quit()
                return
        except Exception as e:
            driver.quit()
            print(keyword+'---搜索失败!')
            print('*'*20)
            print(e)
            print('*'*20)
            return
        cookies={}
        for item in driver.get_cookies():
            cookies[item["name"]]=item["value"]
        self.get_all_company(driver,cookies)
if __name__=='__main__':
    ss=spider(r'重庆公司信息(工商局版)')
    for x in range(19968,40869):
        ss.search_keyword_parser_write('重庆'+chr(x))
